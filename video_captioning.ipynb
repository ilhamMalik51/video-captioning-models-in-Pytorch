{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:10:53.206543Z",
     "start_time": "2021-03-22T11:10:52.752293Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Path,Vocabulary, utility, evaluator and datahandler module\n",
    "from config import Path\n",
    "from dictionary import Vocabulary\n",
    "from utils import Utils\n",
    "from evaluate import Evaluator\n",
    "from data import DataHandler\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#set seed for reproducibility\n",
    "utils = Utils()\n",
    "utils.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:10:54.698930Z",
     "start_time": "2021-03-22T11:10:54.556381Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import configuration and model \n",
    "from config import ConfigMP\n",
    "from models.mean_pooling.model import MeanPooling\n",
    "\n",
    "\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigMP()\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "#cfg.batch_size = 100 #training batch size\n",
    "cfg.n_layers = 1    # number of layers in decoder rnn\n",
    "cfg.decoder_type = 'lstm'  # from {'lstm','gru'}\n",
    "cfg.vocabulary_min_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:10:56.692988Z",
     "start_time": "2021-03-22T11:10:56.670861Z"
    }
   },
   "outputs": [],
   "source": [
    "#Vocabulary object\n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "print('Vocabulary Size : ',voc.num_words) \n",
    "\n",
    "\n",
    "# # Uncomment this block if using vocabulary for the first time or if there is no saved file\n",
    "# text_dict = {}\n",
    "# voc = Vocabulary(cfg)\n",
    "# data_handler = DataHandler(cfg,path,voc)\n",
    "# import json\n",
    "# print(path.feature_file)\n",
    "# json.load(open(path.feature_file))\n",
    "# text_dict.update(data_handler.train_dict)\n",
    "# text_dict.update(data_handler.val_dict)\n",
    "# text_dict.update(data_handler.test_dict)\n",
    "# for k,v in text_dict.items():\n",
    "#     for anno in v:'\n",
    "#         voc.addSentence(anno)\n",
    "# voc.save()\n",
    "\n",
    "\n",
    "##Uncomment this block for filtering Rare Words from Dictionary\n",
    "min_count = cfg.vocabulary_min_count #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders model and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:11:01.874306Z",
     "start_time": "2021-03-22T11:10:58.126859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = MeanPooling(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:51:10.412439Z",
     "start_time": "2021-03-22T10:51:10.044987Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "cfg.encoder_lr = 1e-4\n",
    "cfg.decoder_lr = 1e-3\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "model.update_hyperparameters(cfg)\n",
    "val_loss = []\n",
    "for e in range(1,3001):\n",
    "    loss = model.train_epoch(train_loader,utils)\n",
    "    if e%50 == 0 :\n",
    "        print('Epoch -- >',e,'Loss -->',loss)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,loss))\n",
    "        val_loss.append(model.loss_calculate(val_loader,utils))\n",
    "        #print('beam :',test_evaluator_beam.evaluate(utils,model,e,loss))\n",
    "        #print('semibeam :',test_evaluator_semibeam.evaluate(utils,model,e,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets, mask, max_length,_,_,_= dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt = model.GreedyDecoding(features.to(cfg.device))\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt,scores = model.BeamDecoding(features.to(cfg.device),return_single=False)\n",
    "txt,scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.target_tensor_to_caption(voc,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:02:32.210270Z",
     "start_time": "2021-03-17T11:02:32.062768Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import configuration and model \n",
    "\n",
    "from config import ConfigSALSTM\n",
    "from models.SA_LSTM.model import SALSTM\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigSALSTM(opt_encoder=True)\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "cfg.batch_size = 100 #training batch size\n",
    "cfg.n_layers = 1    # number of layers in decoder rnn\n",
    "cfg.decoder_type = 'lstm'  # from {'lstm','gru'}\n",
    "cfg.dropout = 0.5\n",
    "cfg.opt_param_init = False\n",
    "\n",
    "\n",
    "\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "#Vocabulary object, \n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "\n",
    "min_count = 5 #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)\n",
    "#print('Vocabulary Size : ',voc.num_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:02:37.382514Z",
     "start_time": "2021-03-17T11:02:33.430116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = SALSTM(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join('Saved','sa_lstm_msvd.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:14:23.021770Z",
     "start_time": "2021-03-17T11:02:38.180710Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "cfg.encoder_lr = 1e-4\n",
    "cfg.decoder_lr = 1e-4\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "model.update_hyperparameters(cfg)\n",
    "# lr_scheduler = ReduceLROnPlateau(model.dec_optimizer, mode='min', factor=cfg.lr_decay_gamma,\n",
    "#                                      patience=cfg.lr_decay_patience, verbose=True)\n",
    "for e in range(1,1351):\n",
    "    loss_train = model.train_epoch(train_loader,utils)\n",
    "    #loss_val = model.train_epoch(val_loader,utils)\n",
    "    #lr_scheduler.step(loss_train)\n",
    "    if e%50 == 0 :\n",
    "        print('Epoch -- >',e,'Loss -->',loss_train)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,loss_train))\n",
    "        print('beam :',test_evaluator_beam.evaluate(utils,model,e,loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(val_loader)\n",
    "features, targets, mask, max_length,_,motion_feat,object_feat= dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt,_ = model.GreedyDecoding(features.to(cfg.device))\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.target_tensor_to_caption(voc,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt,scr = model.BeamDecoding(features.to(cfg.device))\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import configuration and model \n",
    "\n",
    "from config import ConfigRecNet\n",
    "from models.RecNet.model import RecNet\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigRecNet()\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "cfg.batch_size = 100 #training batch size\n",
    "cfg.n_layers = 1    # number of layers in decoder rnn\n",
    "cfg.decoder_type = 'lstm'  # from {'lstm','gru'}\n",
    "\n",
    "\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "#Vocabulary object, \n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "min_count = cfg.vocabulary_min_count #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)\n",
    "#print('Vocabulary Size : ',voc.num_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = RecNet(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage-1 Training, Or load model after stage-1 Training.\n",
    "\n",
    "encoder_state_dict_file = os.path.join(path.saved_models_path,'sa_lstm_encoder_msvd.pt')\n",
    "decoder_state_dict_file = os.path.join(path.saved_models_path,'sa_lstm_decoder_msvd.pt')\n",
    "#print(encoder_state_dict_file)\n",
    "model.encoder.load_state_dict(torch.load(encoder_state_dict_file))\n",
    "model.decoder.load_state_dict(torch.load(decoder_state_dict_file))\n",
    "print('greedy :',test_evaluator_greedy.evaluate(utils,model,1350,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage-2 Training\n",
    "cfg.encoder_lr = 1e-3\n",
    "cfg.decoder_lr = 1e-3\n",
    "cfg.global_lr = 1e-3\n",
    "cfg.local_lr = 1e-2\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "cfg.training_stage = 2\n",
    "cfg.lmda = 0.1\n",
    "model.update_hyperparameters(cfg)\n",
    "# lr_scheduler = ReduceLROnPlateau(model.dec_optimizer, mode='min', factor=cfg.lr_decay_gamma,\n",
    "#                                      patience=cfg.lr_decay_patience, verbose=True)\n",
    "for e in range(1,2501):\n",
    "    lloss_train, recloss_train = model.train_epoch(train_loader,utils)\n",
    "    #loss_val = model.train_epoch(val_loader,utils)\n",
    "    #lr_scheduler.step(loss_train)\n",
    "    if e%10 == 0 :\n",
    "        model.encoder.eval()\n",
    "        model.decoder.eval()\n",
    "        model.local_reconstructor.eval()\n",
    "        print('Epoch -- >',e,'Likelihood Loss -->',lloss_train,'Reconstruction Loss -->',recloss_train)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,lloss_train))\n",
    "        print('beam :',test_evaluator_beam.evaluate(utils,model,e,lloss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "features, targets, mask, max_length,_,motion_feat,object_feat= dataiter.next()\n",
    "features.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved/msvd_word2index_dic.p\n",
      "keep_words 3981 / 12596 = 0.3161\n",
      "Vocabulary Size :  3984\n"
     ]
    }
   ],
   "source": [
    "#Import configuration and model \n",
    "\n",
    "\n",
    "from config import ConfigMARN\n",
    "from models.MARN.model import MARN\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigMARN()\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "\n",
    "\n",
    "\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "#Vocabulary object, \n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "min_count = cfg.vocabulary_min_count #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)\n",
    "#print('Vocabulary Size : ',voc.num_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = MARN(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "#test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Generated\n",
      "Epoch -- > 475 Loss --> 2.544153099349823   AC loss ---> 0.03702629804611206\n",
      "{'testlen': 4080, 'reflen': 4074, 'guess': [4080, 3410, 2740, 2070], 'correct': [3299, 1951, 1114, 512]}\n",
      "ratio: 1.0014727540498278\n",
      "greedy : {'Bleu_1': 0.8085784313723509, 'Bleu_2': 0.6801622455242978, 'Bleu_3': 0.5729541775100213, 'Bleu_4': 0.46442417853489154, 'METEOR': 0.3370313251742218, 'ROUGE_L': 0.7106234801658579, 'CIDEr': 0.8365994971494196}\n",
      "Epoch -- > 500 Loss --> 2.4684815886357603   AC loss ---> 0.023752954304218293\n",
      "{'testlen': 3993, 'reflen': 3986, 'guess': [3993, 3323, 2653, 1983], 'correct': [3239, 1941, 1119, 518]}\n",
      "ratio: 1.0017561465125435\n",
      "greedy : {'Bleu_1': 0.8111695467065336, 'Bleu_2': 0.688340647406774, 'Bleu_3': 0.5846552979826141, 'Bleu_4': 0.4779986593072039, 'METEOR': 0.3374856703752535, 'ROUGE_L': 0.7141847661509617, 'CIDEr': 0.863423938631794}\n",
      "Epoch -- > 525 Loss --> 2.4317773076272844   AC loss ---> 0.02150739684700966\n",
      "{'testlen': 4034, 'reflen': 4033, 'guess': [4034, 3364, 2694, 2024], 'correct': [3242, 1916, 1088, 516]}\n",
      "ratio: 1.0002479543761467\n",
      "greedy : {'Bleu_1': 0.8036688150716897, 'Bleu_2': 0.6765631149689835, 'Bleu_3': 0.5696603330088893, 'Bleu_4': 0.4659311892489652, 'METEOR': 0.33601406339993817, 'ROUGE_L': 0.708510980586589, 'CIDEr': 0.8392014433319558}\n",
      "Epoch -- > 550 Loss --> 2.38212996585165   AC loss ---> 0.03081868514418602\n",
      "{'testlen': 4083, 'reflen': 4076, 'guess': [4083, 3413, 2743, 2073], 'correct': [3293, 1945, 1096, 497]}\n",
      "ratio: 1.0017173699703137\n",
      "greedy : {'Bleu_1': 0.8065148175359279, 'Bleu_2': 0.6779502638411503, 'Bleu_3': 0.5684079430759746, 'Bleu_4': 0.4580726637886487, 'METEOR': 0.3326112006359616, 'ROUGE_L': 0.7065921761119903, 'CIDEr': 0.8182650806825904}\n",
      "Epoch -- > 575 Loss --> 2.371163915009308   AC loss ---> 0.02155179426074028\n",
      "{'testlen': 4071, 'reflen': 4064, 'guess': [4071, 3401, 2731, 2061], 'correct': [3262, 1938, 1102, 519]}\n",
      "ratio: 1.0017224409446355\n",
      "greedy : {'Bleu_1': 0.8012773274377791, 'Bleu_2': 0.6757172369785198, 'Bleu_3': 0.5690231862602493, 'Bleu_4': 0.4641088114369028, 'METEOR': 0.33477501287087735, 'ROUGE_L': 0.7070102030908302, 'CIDEr': 0.8435478937058518}\n",
      "Epoch -- > 600 Loss --> 2.4068090742186348   AC loss ---> 0.01790922686457634\n",
      "{'testlen': 4013, 'reflen': 4004, 'guess': [4013, 3343, 2673, 2003], 'correct': [3233, 1909, 1093, 517]}\n",
      "ratio: 1.002247752247502\n",
      "greedy : {'Bleu_1': 0.8056316969845987, 'Bleu_2': 0.6782706868217121, 'Bleu_3': 0.572983918713849, 'Bleu_4': 0.4694174429820985, 'METEOR': 0.3355073467668573, 'ROUGE_L': 0.7065089247736421, 'CIDEr': 0.8327681765226151}\n",
      "Epoch -- > 625 Loss --> 2.323035675523169   AC loss ---> 0.02733965903520584\n",
      "{'testlen': 4236, 'reflen': 4226, 'guess': [4236, 3566, 2896, 2226], 'correct': [3358, 1978, 1126, 523]}\n",
      "ratio: 1.002366303833175\n",
      "greedy : {'Bleu_1': 0.792728989612655, 'Bleu_2': 0.6631088809212585, 'Bleu_3': 0.5550130233343634, 'Bleu_4': 0.44768393759996244, 'METEOR': 0.3355806717379687, 'ROUGE_L': 0.703586736964464, 'CIDEr': 0.8228609001934201}\n",
      "Epoch -- > 650 Loss --> 2.335879625819569   AC loss ---> 0.04072949647903443\n",
      "{'testlen': 4079, 'reflen': 4070, 'guess': [4079, 3409, 2739, 2069], 'correct': [3281, 1953, 1123, 528]}\n",
      "ratio: 1.002211302211056\n",
      "greedy : {'Bleu_1': 0.8043638146602587, 'Bleu_2': 0.6788344647815469, 'Bleu_3': 0.573814912139038, 'Bleu_4': 0.46859454733778316, 'METEOR': 0.3376041918172021, 'ROUGE_L': 0.7074434013227405, 'CIDEr': 0.8185324469935817}\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "cfg.encoder_lr = 1e-4\n",
    "cfg.decoder_lr = 1e-4\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "model.generate_memory(data_handler)\n",
    "print('Memory Generated')\n",
    "model.opt_memory_decoder = True\n",
    "model.update_hyperparameters(cfg)\n",
    "\n",
    "#lr_scheduler = StepLR(model.dec_optimizer,300,gamma=0.1,verbose=False)\n",
    "# lr_scheduler = ReduceLROnPlateau(model.dec_optimizer, mode='min', factor=cfg.lr_decay_gamma,\n",
    "#                                      patience=cfg.lr_decay_patience, verbose=True)\n",
    "for e in range(451,651):\n",
    "    loss_train,ac_loss = model.train_epoch(train_loader,utils)\n",
    "    #loss_val = model.train_epoch(val_loader,utils)\n",
    "    #lr_scheduler.step()\n",
    "    if e%25 == 0 :\n",
    "        print('Epoch -- >',e,'Loss -->',loss_train,'  AC loss --->',ac_loss)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,loss_train))\n",
    "        #print('beam :',test_evaluator_beam.evaluate(utils,model,e,loss_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
