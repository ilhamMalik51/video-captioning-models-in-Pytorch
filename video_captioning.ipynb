{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:10:53.206543Z",
     "start_time": "2021-03-22T11:10:52.752293Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Path,Vocabulary, utility, evaluator and datahandler module\n",
    "from config import Path\n",
    "from dictionary import Vocabulary\n",
    "from utils import Utils\n",
    "from evaluate import Evaluator\n",
    "from data import DataHandler\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#set seed for reproducibility\n",
    "utils = Utils()\n",
    "utils.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:10:54.698930Z",
     "start_time": "2021-03-22T11:10:54.556381Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import configuration and model \n",
    "from config import ConfigMP\n",
    "from models.mean_pooling.model import MeanPooling\n",
    "\n",
    "\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigMP()\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "cfg.batch_size = 100 #training batch size\n",
    "cfg.n_layers = 1    # number of layers in decoder rnn\n",
    "cfg.decoder_type = 'lstm'  # from {'lstm','gru'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:10:56.692988Z",
     "start_time": "2021-03-22T11:10:56.670861Z"
    }
   },
   "outputs": [],
   "source": [
    "#Vocabulary object\n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "print('Vocabulary Size : ',voc.num_words) \n",
    "\n",
    "\n",
    "# # Uncomment this block if using vocabulary for the first time or if there is no saved file\n",
    "# text_dict = {}\n",
    "# voc = Vocabulary(cfg)\n",
    "# data_handler = DataHandler(cfg,path,voc)\n",
    "# import json\n",
    "# print(path.feature_file)\n",
    "# json.load(open(path.feature_file))\n",
    "# text_dict.update(data_handler.train_dict)\n",
    "# text_dict.update(data_handler.val_dict)\n",
    "# text_dict.update(data_handler.test_dict)\n",
    "# for k,v in text_dict.items():\n",
    "#     for anno in v:'\n",
    "#         voc.addSentence(anno)\n",
    "# voc.save()\n",
    "\n",
    "\n",
    "##Uncomment this block for filtering Rare Words from Dictionary\n",
    "min_count = cfg.vocabulary_min_count #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders model and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:11:01.874306Z",
     "start_time": "2021-03-22T11:10:58.126859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = MeanPooling(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:51:10.412439Z",
     "start_time": "2021-03-22T10:51:10.044987Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "cfg.encoder_lr = 1e-4\n",
    "cfg.decoder_lr = 1e-4\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "model.update_hyperparameters(cfg)\n",
    "val_loss = []\n",
    "for e in range(1,3001):\n",
    "    loss = model.train_epoch(train_loader,utils)\n",
    "    if e%25 == 0 :\n",
    "        print('Epoch -- >',e,'Loss -->',loss)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,loss))\n",
    "        val_loss.append(model.loss_calculate(val_loader,utils))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets, mask, max_length,_,_,_= dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt = model.GreedyDecoding(features.to(cfg.device))\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt,scores = model.BeamDecoding(features.to(cfg.device),return_single=False)\n",
    "txt,scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.target_tensor_to_caption(voc,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:02:32.210270Z",
     "start_time": "2021-03-17T11:02:32.062768Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import configuration and model \n",
    "\n",
    "from config import ConfigSALSTM\n",
    "from models.SA_LSTM.model import SALSTM\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigSALSTM()\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "cfg.batch_size = 200 #training batch size\n",
    "cfg.n_layers = 1    # number of layers in decoder rnn\n",
    "cfg.decoder_type = 'lstm'  # from {'lstm','gru'}\n",
    "\n",
    "\n",
    "#Vocabulary object, \n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "\n",
    "min_count = 5 #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)\n",
    "#print('Vocabulary Size : ',voc.num_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:02:37.382514Z",
     "start_time": "2021-03-17T11:02:33.430116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = SALSTM(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T11:14:23.021770Z",
     "start_time": "2021-03-17T11:02:38.180710Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "cfg.encoder_lr = 1e-4\n",
    "cfg.decoder_lr = 1e-4\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "model.update_hyperparameters(cfg)\n",
    "# lr_scheduler = ReduceLROnPlateau(model.dec_optimizer, mode='min', factor=cfg.lr_decay_gamma,\n",
    "#                                      patience=cfg.lr_decay_patience, verbose=True)\n",
    "for e in range(1,2501):\n",
    "    loss_train = model.train_epoch(train_loader,utils)\n",
    "    #loss_val = model.train_epoch(val_loader,utils)\n",
    "    #lr_scheduler.step(loss_train)\n",
    "    if e%25 == 0 :\n",
    "        print('Epoch -- >',e,'Loss -->',loss_train)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,loss_train))\n",
    "        print('beam :',test_evaluator_beam.evaluate(utils,model,e,loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(val_loader)\n",
    "features, targets, mask, max_length,_,motion_feat,object_feat= dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt,_ = model.GreedyDecoding(features.to(cfg.device))\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.target_tensor_to_caption(voc,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr,txt,scr = model.BeamDecoding(features.to(cfg.device))\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import configuration and model \n",
    "\n",
    "from config import ConfigRecNet\n",
    "from models.RecNet.model import RecNet\n",
    "\n",
    "#create Mean pooling object\n",
    "cfg = ConfigRecNet()\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msvd'\n",
    "#creation of path object\n",
    "path = Path(cfg,os.getcwd())\n",
    "\n",
    "#Changing the hyperparameters in configuration object\n",
    "cfg.batch_size = 100 #training batch size\n",
    "cfg.n_layers = 1    # number of layers in decoder rnn\n",
    "cfg.decoder_type = 'lstm'  # from {'lstm','gru'}\n",
    "\n",
    "\n",
    "#Vocabulary object, \n",
    "voc = Vocabulary(cfg)\n",
    "#If vocabulary is already saved or downloaded the saved file\n",
    "voc.load() #comment this if using vocabulary for the first time or with no saved file\n",
    "\n",
    "min_count = 5 #remove all words below count min_count\n",
    "voc.trim(min_count=min_count)\n",
    "print('Vocabulary Size : ',voc.num_words)\n",
    "#print('Vocabulary Size : ',voc.num_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "data_handler = DataHandler(cfg,path,voc)\n",
    "train_dset,val_dset,test_dset = data_handler.getDatasets()\n",
    "train_loader,val_loader,test_loader = data_handler.getDataloader(train_dset,val_dset,test_dset)\n",
    "\n",
    "#Model object\n",
    "model = RecNet(voc,cfg,path)\n",
    "#Evaluator object on test data\n",
    "test_evaluator_greedy = Evaluator(model,test_loader,path,cfg,data_handler.test_dict)\n",
    "test_evaluator_beam = Evaluator(model,test_loader,path,cfg,data_handler.test_dict,decoding_type='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "cfg.encoder_lr = 1e-4\n",
    "cfg.decoder_lr = 1e-4\n",
    "cfg.teacher_forcing_ratio = 1.0\n",
    "cfg.training_stage = 2\n",
    "cfg.lmda = 0.2\n",
    "model.update_hyperparameters(cfg)\n",
    "# lr_scheduler = ReduceLROnPlateau(model.dec_optimizer, mode='min', factor=cfg.lr_decay_gamma,\n",
    "#                                      patience=cfg.lr_decay_patience, verbose=True)\n",
    "for e in range(1,2501):\n",
    "    lloss_train, recloss_train = model.train_epoch(train_loader,utils)\n",
    "    #loss_val = model.train_epoch(val_loader,utils)\n",
    "    #lr_scheduler.step(loss_train)\n",
    "    if e%25 == 0 :\n",
    "        print('Epoch -- >',e,'Likelihood Loss -->',lloss_train,'Reconstruction Loss -->',recloss_train)\n",
    "        print('greedy :',test_evaluator_greedy.evaluate(utils,model,e,lloss_train))\n",
    "        print('beam :',test_evaluator_beam.evaluate(utils,model,e,lloss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "features, targets, mask, max_length,_,motion_feat,object_feat= dataiter.next()\n",
    "features.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
